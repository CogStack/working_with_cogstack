{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a MedCATtrainer project export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mct_analysis import MedcatTrainer_export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MCT exports and MedCAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst_mct_export=[\n",
    "    '../../data/medcattrainer_export/20220817_KCH_export/MedCAT_Export_With_Text_2022-08-17_13_34_54.json',\n",
    "    '../../data/medcattrainer_export/20220817_KCH_export/all_from_medcat_pc_fixed_MedCAT_Export_With_Text_2021-01-15_22_01_45_correct.json'\n",
    "               ]\n",
    "#lst_mct_export = ['../../data/medcattrainer_export/'+'MedCAT_Export_With_Text_2022-07-20_10_58_45.json']  # mct_export .json here\n",
    "mct_model = \"/Users/shek/Documents/medcat_models/medcat_model_pack_v1.2.8\"\n",
    "mct = MedcatTrainer_export(mct_export_paths=lst_mct_export, model_pack_path= mct_model)\n",
    "#mct = MedcatTrainer_export(lst_mct_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model card\n",
    "mct.cat.get_model_card(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look to potentially remove any filters that exisit in the model\n",
    "\"\"\"\n",
    "mct.cat.config.linking['filters']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate MCT export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View all Annotations and Meta-annotations created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load all annotations created\n",
    "anns_df = mct.annotation_df()\n",
    "anns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise all Meta-annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta_annotation summary\n",
    "for col in anns_df.loc[:,'acc':].iloc[:,1:]:\n",
    "    print(anns_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta_annotation summary of combinations\n",
    "for k,v in anns_df.loc[:,'acc':].iloc[:,1:].value_counts().iteritems():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the entire MCT export\n",
    "This includes all names of all projects within the export and the document ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# projects\n",
    "anns_df['project'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents\n",
    "anns_df['document_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to delete\n",
    "# del mct.mct_export['projects'][8]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_summary_df = mct.concept_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performance_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotator stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Stats\n",
    "mct.user_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mct.plot_user_stats(save_fig=True, save_fig_filename='20220817_KCH_user_mct_annotations.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate report\n",
    "All of the above functions added into a single Excel file report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(mct.generate_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mct.generate_report(path='20220817_KCH_mct_report.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Annotations\n",
    "\n",
    "helper function to rename meta_task and meta_task values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename meta annotation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which meta tasks to rename\n",
    "rename_meta_anns = {'Subject/Experiencer':'Subject'}\n",
    "# select which meta values for the corresponding meta tasks.\n",
    "rename_meta_anns_values = {'Subject':{'Relative':'Other'}}\n",
    "# run the renaming\n",
    "mct.rename_meta_anns(meta_anns2rename=rename_meta_anns, meta_ann_values2rename=rename_meta_anns_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df = mct.annotation_df()\n",
    "anns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mct.cat.get_model_card(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check meta models\n",
    "meta_models = list(mct.cat.get_model_card(as_dict=True)['MetaCAT models'].keys())\n",
    "meta_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_ann in meta_models:\n",
    "    print(meta_ann, anns_df[meta_ann].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta annotation performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Tuple, Any, Dict\n",
    "from medcat.tokenizers.meta_cat_tokenizers import TokenizerWrapperBase\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from medcat.utils.meta_cat.ml_utils import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcat.meta_cat import MetaCAT\n",
    "from medcat.config_meta_cat import ConfigMetaCAT\n",
    "from medcat.utils.meta_cat.data_utils import prepare_from_json, encode_category_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_piped_data(data: List, start_ind: int, end_ind: int, device: torch.device, pad_id: int) -> Tuple:\n",
    "    r''' Creates a batch given data and start/end that denote batch size, will also add\n",
    "    padding and move to the right device.\n",
    "    Args:\n",
    "        data (List[List[int], int, Optional[int]]):\n",
    "            Data in the format: [[<[input_ids]>, <cpos>, Optional[int]], ...], the third column is optional\n",
    "            and represents the output label\n",
    "        start_ind (`int`):\n",
    "            Start index of this batch\n",
    "        end_ind (`int`):\n",
    "            End index of this batch\n",
    "        device (`torch.device`):\n",
    "            Where to move the data\n",
    "        pad_id (`int`):\n",
    "            Padding index\n",
    "    Returns:\n",
    "        x ():\n",
    "            Same as data, but subsetted and as a tensor\n",
    "        cpos ():\n",
    "            Center positions for the data\n",
    "    '''\n",
    "    max_seq_len = max([len(x[0]) for x in data])\n",
    "    x = [x[0][0:max_seq_len] + [pad_id]*max(0, max_seq_len - len(x[0])) for x in data[start_ind:end_ind]]\n",
    "    cpos = [x[1] for x in data[start_ind:end_ind]]\n",
    "    y = None\n",
    "    if len(data[0]) == 3:\n",
    "        # Means we have the y column\n",
    "        y = torch.tensor([x[2] for x in data[start_ind:end_ind]], dtype=torch.long).to(device)\n",
    "\n",
    "    x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    cpos = torch.tensor(cpos, dtype=torch.long).to(device)\n",
    "\n",
    "    return x, cpos, y\n",
    "\n",
    "\n",
    "def eval_model(model: nn.Module, data: List, config: ConfigMetaCAT, tokenizer: TokenizerWrapperBase) -> Dict:\n",
    "    r''' Evaluate a trained model on the provided data\n",
    "    Args:\n",
    "        model\n",
    "        data\n",
    "        config\n",
    "    '''\n",
    "    device = torch.device(config.general['device']) # Create a torch device\n",
    "    batch_size_eval = config.general['batch_size_eval']\n",
    "    pad_id = config.model['padding_idx']\n",
    "    ignore_cpos = config.model['ignore_cpos']\n",
    "    class_weights = config.train['class_weights']\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights) # Set the criterion to Cross Entropy Loss\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss() # Set the criterion to Cross Entropy Loss\n",
    "\n",
    "    y_eval = [x[2] for x in data]\n",
    "    num_batches = math.ceil(len(data) / batch_size_eval)\n",
    "    running_loss = []\n",
    "    all_logits = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            x, cpos, y = create_batch_piped_data(data, i*batch_size_eval, (i+1)*batch_size_eval, device=device, pad_id=pad_id)\n",
    "            logits = model(x, cpos, ignore_cpos=ignore_cpos)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            # Track loss and logits\n",
    "            running_loss.append(loss.item())\n",
    "            all_logits.append(logits.detach().cpu().numpy())\n",
    "\n",
    "    #print_report(0, running_loss, all_logits, y=y_eval, name='Eval')\n",
    "\n",
    "    score_average = config.train['score_average']\n",
    "    predictions = np.argmax(np.concatenate(all_logits, axis=0), axis=1)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_eval, predictions, average=score_average)\n",
    "    \n",
    "    \"\"\"examples: Dict = {'FP': {}, 'FN': {}, 'TP': {}}\n",
    "    id2category_value = {v: k for k, v in config.general['category_value2id'].items()}\n",
    "    for i, p in enumerate(predictions):\n",
    "        y = id2category_value[y_eval[i]]\n",
    "        p = id2category_value[p]\n",
    "        c = data[i][1]\n",
    "        tkns = data[i][0]\n",
    "        assert tokenizer.hf_tokenizers is not None\n",
    "        text = tokenizer.hf_tokenizers.decode(tkns[0:c]) + \" <<\"+ tokenizer.hf_tokenizers.decode(tkns[c:c+1]).strip() + \">> \" + \\\n",
    "            tokenizer.hf_tokenizers.decode(tkns[c+1:])\n",
    "        info = \"Predicted: {}, True: {}\".format(p, y)\n",
    "        if p != y:\n",
    "            # We made a mistake\n",
    "            examples['FN'][y] = examples['FN'].get(y, []) + [(info, text)]\n",
    "            examples['FP'][p] = examples['FP'].get(p, []) + [(info, text)]\n",
    "        else:\n",
    "            examples['TP'][y] = examples['TP'].get(y, []) + [(info, text)]\n",
    "\"\"\"\n",
    "    return predictions#{'predictions':predictions,'precision': precision, 'recall': recall, 'f1': f1, 'examples': examples}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval(metacat_model, mct_export):    \n",
    "    g_config = metacat_model.config.general\n",
    "    t_config = metacat_model.config.train\n",
    "    #t_config['test_size'] = 0\n",
    "    t_config['shuffle_data']= False\n",
    "    t_config['prerequisites']={}\n",
    "    t_config['cui_filter']={}    \n",
    "                \n",
    "    with open(mct_export, 'r') as f:\n",
    "        data_loaded: Dict = json.load(f)\n",
    "\n",
    "    # Prepare the data\n",
    "    assert metacat_model.tokenizer is not None\n",
    "    data = prepare_from_json(data_loaded, g_config['cntx_left'], g_config['cntx_right'], metacat_model.tokenizer,\n",
    "                             cui_filter=t_config['cui_filter'],\n",
    "                             replace_center=g_config['replace_center'], prerequisites=t_config['prerequisites'],\n",
    "                             lowercase=g_config['lowercase'])\n",
    "\n",
    "    # Check is the name there\n",
    "    category_name = g_config['category_name']\n",
    "    if category_name not in data:\n",
    "        warnings.warn(f\"The meta_model {category_name} does not exist in this MedCATtrainer export.\", UserWarning)\n",
    "        return {category_name:f\"{category_name} does not exist\"}\n",
    "\n",
    "    data = data[category_name]\n",
    "\n",
    "    # We already have everything, just get the data\n",
    "    category_value2id = g_config['category_value2id']\n",
    "    data, _ = encode_category_values(data, existing_category_value2id=category_value2id)\n",
    "    print(_)\n",
    "    print(len(data))\n",
    "    # Run evaluation\n",
    "    assert metacat_model.tokenizer is not None\n",
    "    result = eval_model(metacat_model.model, data, config=metacat_model.config, tokenizer=metacat_model.tokenizer)\n",
    "\n",
    "    return {'predictions': result, 'meta_values':_}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_df=anns_df[(anns_df['validated']==True)&(anns_df['deleted']==False)&(anns_df['killed']==False)&(anns_df['irrelevant']==False)]\n",
    "meta_df=meta_df.reset_index(drop=True)\n",
    "for meta_model in meta_models:\n",
    "    print(f'Checking metacat model: {meta_model}')\n",
    "    _meta_model = MetaCAT.load(mct_model+'/meta_'+meta_model)\n",
    "    meta_results=eval(_meta_model, '../../data/medcattrainer_export/test.json')\n",
    "    _meta_values = { v:k for k,v in meta_results['meta_values'].items()}\n",
    "    print(_meta_values)\n",
    "    pred_meta_values = []\n",
    "    counter = 0\n",
    "    for meta_value in meta_df[meta_model]:\n",
    "        if pd.isnull(meta_value):\n",
    "            pred_meta_values.append(np.nan)\n",
    "        else:\n",
    "            pred_meta_values.append(_meta_values.get(meta_results['predictions'][counter],np.nan))\n",
    "            counter+=1\n",
    "    meta_df.insert(meta_df.columns.get_loc(meta_model)+1,'predict_'+meta_model,pred_meta_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['Subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['predict_Subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta annotation summary stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_df[meta_task].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_task = 'Presence'\n",
    "meta_task_values = meta_df[meta_task].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_df[meta_df[meta_task] == 'True']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_task_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in meta_task_values:\n",
    "    temp_df = meta_df[meta_df[meta_task] == task][[meta_task,'predict_'+meta_task]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model = MetaCAT.load(mct_model+'/meta_'+meta_model)\n",
    "    \n",
    "meta_results=eval(_meta_model, '../../data/medcattrainer_export/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "meta_df=anns_df[(anns_df['validated']==True)&(anns_df['deleted']==False)&(anns_df['killed']==False)&(anns_df['irrelevant']==False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = list(results[0].values())[0]['predictions']\n",
    "\n",
    "\n",
    "\n",
    "classe = {\"False\": 2,\"Hypothetical\": 1,\"True\": 0}\n",
    "classes = { v:k for k,v in classe.items()}\n",
    "pred_meta = []\n",
    "\n",
    "counter = 0\n",
    "for meta_value in meta_df['Presence']:\n",
    "    if pd.isnull(meta_value):\n",
    "        pred_meta.append(np.nan)\n",
    "    else:\n",
    "        pred_meta.append(classes.get(test_results[counter],np.nan))\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = {\"False\": 2,\"Hypothetical\": 1,\"True\": 0}\n",
    "classes = { v:k for k,v in classe.items()}\n",
    "test = [classes.get(i, np.nan) for i in list(results[0].values())[0]['predictions'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(results[0].values())[0]['predictions']))\n",
    "print(len(list(results[1].values())[0]['predictions']))\n",
    "print(len(list(results[2].values())[0]['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(results[0].values())[0]['y_eval']))\n",
    "print(len(list(results[1].values())[0]['y_eval']))\n",
    "print(len(list(results[2].values())[0]['y_eval']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anns_df['Presence'].unique())\n",
    "print(anns_df['Subject'].unique())\n",
    "print(anns_df['Time'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anns_df['Presence'].isnull().sum())\n",
    "print(anns_df['Subject'].isnull().sum())\n",
    "print(anns_df['Time'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anns_df['Subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(results[0].values())[0]['predictions'])-anns_df['Presence'].isnull().sum())\n",
    "print(len(list(results[1].values())[0]['predictions'])-anns_df['Subject'].isnull().sum())\n",
    "print(len(list(results[2].values())[0]['predictions'])-anns_df['Time'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1015-test_df['Presence'].isnull().sum())\n",
    "print(1015-test_df['Subject'].isnull().sum())\n",
    "print(1015-test_df['Time'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model = MetaCAT.load(mct_model+'/meta_'+'Presence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model.config.general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model.config.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model.config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model.config.general['device'] # Create a torch device\n",
    "_meta_model.config.general['batch_size_eval']\n",
    "_meta_model.config.model['padding_idx']\n",
    "_meta_model.config.model['ignore_cpos']\n",
    "_meta_model.config.train['class_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = {\"False\": 2,\"Hypothetical\": 1,\"True\": 0}\n",
    "classes = { v:k for k,v in classe.items()}\n",
    "test = [classes.get(i, np.nan) for i in presence_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.get(1, 'Nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [classes.get(i, np.nan) for i in presence_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anns_df['Presence'])-878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df[(anns_df['correct']==True)|(anns_df['alternative']==True)&(anns_df['deleted']==False)&(anns_df['killed']==False)&(anns_df['irrelevant']==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df['alternative'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df['deleted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df['killed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df['irrelevant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results[0].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_from_json(data: Dict,\n",
    "                      cntx_left: int,\n",
    "                      cntx_right: int,\n",
    "                      tokenizer: TokenizerWrapperBase,\n",
    "                      cui_filter: Optional[set] = None,\n",
    "                      replace_center: Optional[str] = None,\n",
    "                      prerequisites: Dict = {},\n",
    "                      lowercase: bool = True) -> Dict:\n",
    "    \"\"\" Convert the data from a json format into a CSV-like format for training. This function is not very efficient (the one\n",
    "    working with spacy documents as part of the meta_cat.pipe method is much better). If your dataset is > 1M documents think\n",
    "    about rewriting this function - but would be strange to have more than 1M manually annotated documents.\n",
    "\n",
    "    Args:\n",
    "        data (`dict`):\n",
    "            Loaded output of MedCATtrainer. If we have a `my_export.json` from MedCATtrainer, than data = json.load(<my_export>).\n",
    "        cntx_left (`int`):\n",
    "            Size of context to get from the left of the concept\n",
    "        cntx_right (`int`):\n",
    "            Size of context to get from the right of the concept\n",
    "        tokenizer (`medcat.tokenizers.meta_cat_tokenizers`):\n",
    "            Something to split text into tokens for the LSTM/BERT/whatever meta models.\n",
    "        replace_center (`str`, optional):\n",
    "            If not None the center word (concept) will be replaced with whatever this is.\n",
    "        prerequisites (`dict`, optional):\n",
    "            A map of prerequisities, for example our data has two meta-annotations (experiencer, negation). Assume I want to create\n",
    "            a dataset for `negation` but only in those cases where `experiencer=patient`, my prerequisites would be:\n",
    "                {'Experiencer': 'Patient'} - Take care that the CASE has to match whatever is in the data\n",
    "        lowercase (`bool`, defaults to True):\n",
    "            Should the text be lowercased before tokenization\n",
    "\n",
    "    Returns:\n",
    "        out_data (`dict`):\n",
    "            Example: {'category_name': [('<category_value>', '<[tokens]>', '<center_token>'), ...], ...}\n",
    "    \"\"\"\n",
    "    out_data: Dict = {}\n",
    "    for project in data['projects']:\n",
    "        for document in project['documents']:\n",
    "            text = str(document['text'])\n",
    "            if lowercase:\n",
    "                text = text.lower()\n",
    "\n",
    "            if len(text) > 0:\n",
    "                doc_text = tokenizer(text)\n",
    "\n",
    "                for ann in document.get('annotations', document.get('entities', {}).values()): # A hack to suport entities and annotations\n",
    "                    cui = ann['cui']\n",
    "                    skip = False\n",
    "                    if 'meta_anns' in ann and prerequisites:\n",
    "                        # It is possible to require certain meta_anns to exist and have a specific value\n",
    "                        for meta_ann in prerequisites:\n",
    "                            if meta_ann not in ann['meta_anns'] or ann['meta_anns'][meta_ann]['value'] != prerequisites[meta_ann]:\n",
    "                                # Skip this annotation as the prerequisite is not met\n",
    "                                skip = True\n",
    "                                break\n",
    "\n",
    "                    if not skip and (cui_filter is None or not cui_filter or cui in cui_filter):\n",
    "                        if ann.get('validated', True) and (not ann.get('deleted', False) and not ann.get('killed', False)\n",
    "                                                           and not ann.get('irrelevant', False)):\n",
    "                            start = ann['start']\n",
    "                            end = ann['end']\n",
    "\n",
    "                            # Get the index of the center token\n",
    "                            ind = 0\n",
    "                            for ind, pair in enumerate(doc_text['offset_mapping']):\n",
    "                                if start >= pair[0] and start < pair[1]:\n",
    "                                    break\n",
    "\n",
    "                            _start = max(0, ind - cntx_left)\n",
    "                            _end = min(len(doc_text['input_ids']), ind + 1 + cntx_right)\n",
    "                            tkns = doc_text['input_ids'][_start:_end]\n",
    "                            cpos = cntx_left + min(0, ind-cntx_left)\n",
    "\n",
    "                            if replace_center is not None:\n",
    "                                if lowercase:\n",
    "                                    replace_center = replace_center.lower()\n",
    "                                for p_ind, pair in enumerate(doc_text['offset_mapping']):\n",
    "                                    if start >= pair[0] and start < pair[1]:\n",
    "                                        s_ind = p_ind\n",
    "                                    if end > pair[0] and end <= pair[1]:\n",
    "                                        e_ind = p_ind\n",
    "\n",
    "                                ln = e_ind - s_ind\n",
    "                                tkns = tkns[:cpos] + tokenizer(replace_center)['input_ids'] + tkns[cpos+ln+1:]\n",
    "\n",
    "                            # Backward compatibility if meta_anns is a list vs dict in the new approach\n",
    "                            meta_anns = []\n",
    "                            if 'meta_anns' in ann:\n",
    "                                meta_anns = ann['meta_anns'].values() if type(ann['meta_anns']) == dict else ann['meta_anns']\n",
    "\n",
    "                            # If the annotation is validated\n",
    "                            for meta_ann in meta_anns:\n",
    "                                name = meta_ann['name']\n",
    "                                value = meta_ann['value']\n",
    "\n",
    "                                sample = [tkns, cpos, value]\n",
    "\n",
    "                                if name in out_data:\n",
    "                                    out_data[name].append(sample)\n",
    "                                else:\n",
    "                                    out_data[name] = [sample]\n",
    "\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = prepare_from_json(mct.mct_export, cntx_left=_meta_model.config.general['cntx_left'],\n",
    "                         cntx_right= _meta_model.config.general['cntx_right'],\n",
    "                         tokenizer=_meta_model.tokenizer, cui_filter={},prerequisites={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test['Subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test['Presence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mct.mct_export['projects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model.config.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_df['Subject/Experiencer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model = MetaCAT.load(mct_model+'/meta_'+meta_model)\n",
    "_meta_model.config.train['prerequisites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model.config.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_meta_model.config.general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_config = metacat_model.config.general\n",
    "t_config = metacat_model.config.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_config = _.config.general\n",
    "t_config = _.config.train\n",
    "p =_.config.train['prerequisites']={}\n",
    "data = prepare_from_json(mct.mct_export, g_config['cntx_left'], g_config['cntx_right'], _.tokenizer,\n",
    "                         cui_filter=t_config['cui_filter'], replace_center=g_config['replace_center'], prerequisites=p,\n",
    "                         lowercase=g_config['lowercase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.config.train['prerequisites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Presence = MetaCAT.load(mct_model+'/meta_'+meta_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Presence.config.train['prerequisites']={}\n",
    "results_presence = test_Presence.eval(mct_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject = MetaCAT.load(mct_model+'/meta_'+meta_models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject = MetaCAT.load(mct_model+'/meta_'+meta_models[1])\n",
    "test_subject.config.general['category_name'] = 'Subject/Experiencer'\n",
    "test_subject.config.train['prerequisites']={}\n",
    "results_subject = test_subject.eval(mct_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time = MetaCAT.load(mct_model+'/meta_'+meta_models[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time.config.train['prerequisites']={}\n",
    "results_time =test_time.eval(mct_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_models[2] == test_time.config.general['category_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time.config.general['category_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from medcat.utils.meta_cat.ml_utils import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_utils.eval_model(model=test_time.model,\n",
    "                    config=test_time.config,\n",
    "                    data=mct_export,\n",
    "                    tokenizer=test_time.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_from_json(data: Dict,\n",
    "                      cntx_left: int,\n",
    "                      cntx_right: int,\n",
    "                      tokenizer: TokenizerWrapperBase,\n",
    "                      cui_filter: Optional[set] = None,\n",
    "                      replace_center: Optional[str] = None,\n",
    "                      prerequisites: Dict = {},\n",
    "                      lowercase: bool = True) -> Dict:\n",
    "    \"\"\" Convert the data from a json format into a CSV-like format for training. This function is not very efficient (the one\n",
    "    working with spacy documents as part of the meta_cat.pipe method is much better). If your dataset is > 1M documents think\n",
    "    about rewriting this function - but would be strange to have more than 1M manually annotated documents.\n",
    "\n",
    "    Args:\n",
    "        data (`dict`):\n",
    "            Loaded output of MedCATtrainer. If we have a `my_export.json` from MedCATtrainer, than data = json.load(<my_export>).\n",
    "        cntx_left (`int`):\n",
    "            Size of context to get from the left of the concept\n",
    "        cntx_right (`int`):\n",
    "            Size of context to get from the right of the concept\n",
    "        tokenizer (`medcat.tokenizers.meta_cat_tokenizers`):\n",
    "            Something to split text into tokens for the LSTM/BERT/whatever meta models.\n",
    "        replace_center (`str`, optional):\n",
    "            If not None the center word (concept) will be replaced with whatever this is.\n",
    "        prerequisites (`dict`, optional):\n",
    "            A map of prerequisities, for example our data has two meta-annotations (experiencer, negation). Assume I want to create\n",
    "            a dataset for `negation` but only in those cases where `experiencer=patient`, my prerequisites would be:\n",
    "                {'Experiencer': 'Patient'} - Take care that the CASE has to match whatever is in the data\n",
    "        lowercase (`bool`, defaults to True):\n",
    "            Should the text be lowercased before tokenization\n",
    "\n",
    "    Returns:\n",
    "        out_data (`dict`):\n",
    "            Example: {'category_name': [('<category_value>', '<[tokens]>', '<center_token>'), ...], ...}\n",
    "    \"\"\"\n",
    "    out_data: Dict = {}\n",
    "\n",
    "    for project in data['projects']:\n",
    "        for document in project['documents']:\n",
    "            text = str(document['text'])\n",
    "            if lowercase:\n",
    "                text = text.lower()\n",
    "\n",
    "            if len(text) > 0:\n",
    "                doc_text = tokenizer(text)\n",
    "\n",
    "                for ann in document.get('annotations', document.get('entities', {}).values()): # A hack to suport entities and annotations\n",
    "                    cui = ann['cui']\n",
    "                    skip = False\n",
    "                    if 'meta_anns' in ann and prerequisites:\n",
    "                        # It is possible to require certain meta_anns to exist and have a specific value\n",
    "                        for meta_ann in prerequisites:\n",
    "                            if meta_ann not in ann['meta_anns'] or ann['meta_anns'][meta_ann]['value'] != prerequisites[meta_ann]:\n",
    "                                # Skip this annotation as the prerequisite is not met\n",
    "                                skip = True\n",
    "                                break\n",
    "\n",
    "                    if not skip and (cui_filter is None or not cui_filter or cui in cui_filter):\n",
    "                        if ann.get('validated', True) and (not ann.get('deleted', False) and not ann.get('killed', False)\n",
    "                                                           and not ann.get('irrelevant', False)):\n",
    "                            start = ann['start']\n",
    "                            end = ann['end']\n",
    "\n",
    "                            # Get the index of the center token\n",
    "                            ind = 0\n",
    "                            for ind, pair in enumerate(doc_text['offset_mapping']):\n",
    "                                if start >= pair[0] and start < pair[1]:\n",
    "                                    break\n",
    "\n",
    "                            _start = max(0, ind - cntx_left)\n",
    "                            _end = min(len(doc_text['input_ids']), ind + 1 + cntx_right)\n",
    "                            tkns = doc_text['input_ids'][_start:_end]\n",
    "                            cpos = cntx_left + min(0, ind-cntx_left)\n",
    "\n",
    "                            if replace_center is not None:\n",
    "                                if lowercase:\n",
    "                                    replace_center = replace_center.lower()\n",
    "                                for p_ind, pair in enumerate(doc_text['offset_mapping']):\n",
    "                                    if start >= pair[0] and start < pair[1]:\n",
    "                                        s_ind = p_ind\n",
    "                                    if end > pair[0] and end <= pair[1]:\n",
    "                                        e_ind = p_ind\n",
    "\n",
    "                                ln = e_ind - s_ind\n",
    "                                tkns = tkns[:cpos] + tokenizer(replace_center)['input_ids'] + tkns[cpos+ln+1:]\n",
    "\n",
    "                            # Backward compatibility if meta_anns is a list vs dict in the new approach\n",
    "                            meta_anns = []\n",
    "                            if 'meta_anns' in ann:\n",
    "                                meta_anns = ann['meta_anns'].values() if type(ann['meta_anns']) == dict else ann['meta_anns']\n",
    "\n",
    "                            # If the annotation is validated\n",
    "                            for meta_ann in meta_anns:\n",
    "                                name = meta_ann['name']\n",
    "                                value = meta_ann['value']\n",
    "\n",
    "                                sample = [tkns, cpos, value]\n",
    "\n",
    "                                if name in out_data:\n",
    "                                    out_data[name].append(sample)\n",
    "                                else:\n",
    "                                    out_data[name] = [sample]\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_config = test_time.config.general\n",
    "t_config = test_time.config.train['prerequisites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_from_json(data_loaded, g_config['cntx_left'], g_config['cntx_right'], test_time.tokenizer,\n",
    "                         cui_filter=t_config['cui_filter'],\n",
    "                         replace_center=g_config['replace_center'], prerequisites={},\n",
    "                         lowercase=g_config['lowercase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = g_config['category_name']\n",
    "if category_name not in data:\n",
    "    raise Exception(\"The category name does not exist in this json file.\")\n",
    "data = data[category_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcat.utils.meta_cat.ml_utils import predict, train_model, set_all_seeds, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_value2id = g_config['category_value2id']\n",
    "data, _ = encode_category_values(data, existing_category_value2id=category_value2id)\n",
    "\n",
    "# Run evaluation\n",
    "assert test_time.tokenizer is not None\n",
    "result = eval_model(test_time.model, data, config=test_time.config, tokenizer=test_time.tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
