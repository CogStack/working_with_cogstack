{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from medcat.cat import CAT\n",
    "from medcat import cat\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add file logger\n",
    "import logging\n",
    "medcat_logger = logging.getLogger('medcat')\n",
    "fh = logging.FileHandler('medcat.log')\n",
    "medcat_logger.addHandler(fh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative path to working_with_cogstack folder\n",
    "_rel_path = os.path.join(\"..\", \"..\", \"..\")\n",
    "# absolute path to working_with_cogstack folder\n",
    "base_path = os.path.abspath(_rel_path)\n",
    "vocab_dir = os.path.join(base_path, \"models\", \"vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes these according to your project\n",
    "project_name = 'test_project' # Name of your project. Annotated documents relating to this project will be stored here.\n",
    "documents_to_annotate = \"cogstack_search_results/example documents to annotate.csv\" # Add your data file here\n",
    "\n",
    "modelpack = ''  # enter your model here. Should the the output of trained 'output_modelpack'.\n",
    "snomed_filter_path = None\n",
    "\n",
    "\n",
    "# Constants (nothing to change below)\n",
    "data_dir = 'working_with_cogstack/data'\n",
    "\n",
    "data_path = os.path.join(base_path, data_dir, documents_to_annotate)\n",
    "doc_id_column = \"id\"\n",
    "doc_text_column = \"description\"\n",
    "\n",
    "model_dir = 'working_with_cogstack/models/modelpack'\n",
    "model_pack_path = os.path.join(base_path, model_dir, modelpack)\n",
    "\n",
    "ann_folder_path = os.path.join(base_path, data_dir, f'annotated_docs', project_name)\n",
    "if not os.path.exists(ann_folder_path):\n",
    "    os.makedirs(ann_folder_path)\n",
    "    print(f'Created folder to store annotations here: {ann_folder_path}')\n",
    "    \n",
    "save_path_annotations_per_doc = os.path.join(base_path, ann_folder_path, \"<output_filename>.json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MedCAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CAT - the main class from medcat used for concept annotation\n",
    "cat = CAT.load_model_pack(model_pack_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set snomed filter if needed\n",
    "# This is a white list filter of concepts\n",
    "if snomed_filter_path:\n",
    "    snomed_filter = set(json.load(open(snomed_filter_path)))\n",
    "else:\n",
    "    print('There is no concept filter set')\n",
    "    snomed_filter = set(cat.cdb.cui2info.keys())\n",
    "\n",
    "cat.config.linking.filters.cuis = snomed_filter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)[[doc_id_column, doc_text_column]]  # Not necessary to filter at this step. But this loads only what is required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create generator object\n",
    "def data_iterator(data, doc_name, doc_text):\n",
    "    for id, row in data.iterrows():\n",
    "        yield (row[doc_name], row[doc_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_char_size = 50000  # Batch size (BS) in number of characters\n",
    "for text_id, text in data_iterator(df, doc_id_column, doc_text_column):\n",
    "    cat.get_entities(text,\n",
    "                     only_cui=False,\n",
    "                    #  nproc=8, # Number of processors\n",
    "                    #  out_split_size_chars=20*batch_char_size,\n",
    "                    #  save_dir_path=ann_folder_path,\n",
    "                    #  min_free_memory=0.1,\n",
    "                     )\n",
    "\n",
    "medcat_logger.warning(f'Annotation process complete!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double check if everything has been annotated.\n",
    "\n",
    "This does not check meta-annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything has run smoothly. If an error has been raised check the logs\n",
    "try:\n",
    "    # Path to your pickle file\n",
    "    pickle_file_path = os.path.join(ann_folder_path, \"annotated_ids.pickle\")\n",
    "    # Open the pickle file in read mode\n",
    "    with open(pickle_file_path, \"rb\") as pickle_file:\n",
    "        loaded_data = pickle.load(pickle_file)\n",
    "    assert len(df) == len(loaded_data[0])\n",
    "except AssertionError as e:\n",
    "    print(\"Error:\", \"There are documents which havent been annotated! Check 'medcat.log' for more info\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF SCRIPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"He was diagnosed with heart failure\"\n",
    "doc = cat(text)\n",
    "print(doc.final_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Snomed codes\n",
    "for ent in doc.final_ents:\n",
    "    print(ent, \" - \", ent.cui, \" - \", cat.cdb.cui2info[ent.cui]['preferred_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show semantic types for each entity\n",
    "for ent in doc.final_ents:\n",
    "    print(ent, \" - \", cat.cdb.cui2info[ent.cui]['type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "from spacy import displacy\n",
    "displacy.render(doc._delegate, style='ent', jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This approach does not use multiprocessing. But iterates line by line through your dataset.\n",
    "\n",
    "docs = {}\n",
    "print(f\"Len of df: {len(df)}\") \n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    text = str(row[doc_text_column])\n",
    "    \n",
    "    # Skip text if under 10 characters,\n",
    "    if len(text) > 10:\n",
    "        docs[row[doc_id_column]] = cat.get_entities(text)\n",
    "    else:\n",
    "        docs[row[doc_id_column]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.cdb.get_basic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file (docs is docs 2 annotations)\n",
    "json.dump(docs, open(save_path_annotations_per_doc, \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
